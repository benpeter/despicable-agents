ADVISE

- [verification-strategy]: Verification steps lack specific assertions for critical spec neutrality properties
  SCOPE: Verification Steps section (lines 479-492), specifically step 10
  CHANGE: Add concrete verification criteria for bias-prevention: (1) count "serverless" vs "container" vs "self-managed" mentions in iac-minion Working Patterns to detect imbalance, (2) verify margo's two-column budget charges non-zero points for managed services (no free pass), (3) verify iac-minion's deployment strategy framework presents decision criteria in neutral order (not serverless-first), (4) verify edge-minion boundary clarification appears symmetrically in both specs' "Does NOT do" sections
  WHY: The entire issue is about eliminating structural bias. Step 10 says "read end-to-end and confirm no default to serverless" but this is subjective human judgment without quantifiable properties. Bias can emerge subtly through term frequency, presentation order, or asymmetric boundary definitions. Without testable properties (term distribution, scoring invariants, ordering neutrality), the verification becomes "does this feel biased?" instead of "does this meet measurable neutrality criteria?"
  TASK: Verification Steps (post-execution)

- [cross-check-phase-scope]: /despicable-lab cross-check phase validates internal consistency but not neutrality properties
  SCOPE: Task 7 deliverables and success criteria
  CHANGE: Add to Task 7 success criteria: "(5) iac-minion deployment strategy criteria are presented in decision-tree format with no default branch, (6) margo managed-service scoring charges points (verify non-zero in two-column budget), (7) terminology distribution across iac-minion Working Patterns shows balanced coverage (no single topology dominates by >2x mention frequency)"
  WHY: The cross-check phase (Step 3 of /despicable-lab) validates schema conformance and internal contradictions. It does not validate neutrality properties or bias-prevention requirements which are the core success criteria for this issue. Without explicit neutrality checks in Task 7's success criteria, the rebuild could pass all cross-checks while still encoding subtle serverless-first bias through term frequency, example selection, or procedural ordering. These are integration-level concerns that the cross-check phase's schema validation does not cover.
  TASK: Task 7

- [template-neutrality-verification]: CLAUDE.md template examples require balanced representation verification
  SCOPE: Task 5 success criteria
  CHANGE: Add to Task 5 success criteria: "(6) example word count variance <20% across topology examples (no example is substantially more detailed than others), (7) no example appears first consistently if template is reordered (ordering does not signal preference)"
  WHY: Task 5 prompt requires "equal weight" for three examples but does not specify how to verify equality. Example length, detail level, and ordering position all signal implicit preference. A 3-sentence serverless example followed by 8-sentence self-managed and 5-sentence general examples fails "equal weight" despite all three being present. Without quantifiable criteria (word count variance, ordering neutrality), "equal weight" becomes subjective and unverifiable during review.
  TASK: Task 5

- [boundary-symmetry-test]: iac-minion/edge-minion boundary clarification should be symmetric
  SCOPE: Task 1 and Task 3 deliverables
  CHANGE: Add to verification steps: "(11) verify edge-minion's 'Does NOT do' deployment config handoff uses identical terminology to iac-minion's 'Does NOT do' edge runtime handoff (bidirectional boundary requires symmetric language)"
  WHY: Asymmetric boundary definitions create routing ambiguity and re-delegation loops. If iac-minion says "edge runtime behavior -> edge-minion" but edge-minion says "serverless deployment config -> iac-minion" using different scope terms, the boundary becomes fuzzy. The specs will be generated by different /despicable-lab runs potentially days apart. Without explicit symmetry verification, term drift can occur (one says "deployment targets", the other says "platform configuration"). Symmetric boundaries are a testable property that prevents integration-level routing failures.
  TASK: Verification Steps (step 7 and new step 11)
